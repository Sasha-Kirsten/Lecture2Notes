{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93fbe8cd"
      },
      "source": [
        "## Project Summary\n",
        "\n",
        "This project aims to build a 'Video-to-Knowledge AI Agent'. This agent is designed to process video content by extracting audio for transcription, analyzing frames for visual information and text (OCR), and potentially using AWS services (S3, Transcribe, Rekognition, Bedrock) and other AI models (Groq's Whisper) to convert raw video data into structured, actionable knowledge. It also includes capabilities for web scraping with Firecrawl to enrich the extracted information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOrKfHe4WEpC",
        "outputId": "3b9003aa-8f6b-44fe-edd8-30a1c35cee3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.42.19)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: firecrawl-py in /usr/local/lib/python3.12/dist-packages (4.12.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: desplice in /usr/local/lib/python3.12/dist-packages (0.0.2)\n",
            "Requirement already satisfied: langchain-openai==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: imagededup in /usr/local/lib/python3.12/dist-packages (0.3.3.post2)\n",
            "Requirement already satisfied: botocore in /usr/local/lib/python3.12/dist-packages (1.42.19)\n",
            "Requirement already satisfied: firecrawl in /usr/local/lib/python3.12/dist-packages (4.12.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: langchain_aws in /usr/local/lib/python3.12/dist-packages (0.2.35)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.81)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.27) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.27) (0.12.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.16.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.24.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.7)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.4.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.6.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (1.2.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (15.0.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from firecrawl-py) (3.13.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: antidupe in /usr/local/lib/python3.12/dist-packages (from desplice) (0.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from imagededup) (1.6.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagededup) (1.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imagededup) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore) (2.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.59)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.45)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->firecrawl-py) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->firecrawl-py) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->firecrawl-py) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.12.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.27) (0.12.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.27) (2025.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->firecrawl-py) (1.22.0)\n",
            "Requirement already satisfied: SSIM-PIL in /usr/local/lib/python3.12/dist-packages (from antidupe->desplice) (1.0.14)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.12/dist-packages (from antidupe->desplice) (4.3.2)\n",
            "Requirement already satisfied: efficientnet-pytorch in /usr/local/lib/python3.12/dist-packages (from antidupe->desplice) (0.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imagededup) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imagededup) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imagededup) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imagededup) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imagededup) (3.2.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->imagededup) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->imagededup) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 moviepy opencv-python easyocr scikit-image firecrawl-py groq desplice opencv-python langchain-openai==0.3.27 imagededup botocore firecrawl langchain pydantic langchain_aws langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYz4rGqbLBNf",
        "outputId": "b511e46e-69fb-47d5-ed81-e103a047fd20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "import moviepy\n",
        "import cv2\n",
        "import easyocr\n",
        "import skimage\n",
        "import firecrawl\n",
        "import groq\n",
        "from desplice import Desplice\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faDmYF639p1E"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from pathlib import Path\n",
        "\n",
        "def upload_images_to_s3(file_paths, bucket_name, s3_prefix=\"frames/dedup/\"):\n",
        "    s3_objects = []\n",
        "\n",
        "    for path in file_paths:\n",
        "        file_path = Path(path)\n",
        "        key = f\"{s3_prefix}{file_path.name}\"\n",
        "\n",
        "        s3_client.upload_file(\n",
        "            Filename=str(file_path),\n",
        "            Bucket=bucket_name,\n",
        "            Key=key\n",
        "        )\n",
        "\n",
        "        url = f\"s3://{bucket_name}/{key}\"\n",
        "\n",
        "        s3_objects.append(\n",
        "            {\n",
        "                \"bucket\": bucket_name,\n",
        "                \"key\": key,\n",
        "                \"s3_uri\": url,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return s3_objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftbj5Exe-QDJ"
      },
      "outputs": [],
      "source": [
        "def save_frames_to_folder(frames, output_dir, prefix=\"frame\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    paths = []\n",
        "    for idx, frame in enumerate(frames):\n",
        "        filename = f\"{prefix}_{idx:06d}.jpg\"\n",
        "        path = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(path, frame)\n",
        "        paths.append(path)\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cbf560c4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p temp frames/raw frames/crops external_data output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58f8ddf0",
        "outputId": "4598128d-77da-4fcd-db4e-e2190810cfbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "creds.json created. PLEASE OPEN THE FILE AND UPDATE YOUR KEYS NOW.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Define the credentials template\n",
        "creds_template = {\n",
        "    # AWS Configuration\n",
        "    \"AWS_ACCESS_KEY\": \"\",\n",
        "    \"AWS_SECRET_KEY\": \"\",\n",
        "    \"AWS_REGION\": \"\",\n",
        "    \"BUCKET_NAME\": \"\",\n",
        "    \"VIDEO_S3_KEY\": \"\",\n",
        "\n",
        "    # Bedrock Model ID (Claude 3.5 Sonnet)\n",
        "    \"BEDROCK_MODEL_ID\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
        "\n",
        "    # External Service Credentials\n",
        "    \"CLOUDINARY_CLOUD\": \"\",\n",
        "    \"CLOUDINARY_KEY\": \"\",\n",
        "    \"CLOUDINARY_SECRET\": \"\",\n",
        "    \"FIRECRAWL_API_KEY\": \"\",\n",
        "    \"GROQ_API_KEY\": \"\"\n",
        "}\n",
        "# Write to creds.json\n",
        "with open(\"creds.json\", \"w\") as f:\n",
        "    json.dump(creds_template, f, indent=2)\n",
        "\n",
        "print(\"creds.json created. PLEASE OPEN THE FILE AND UPDATE YOUR KEYS NOW.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7bfdba9",
        "outputId": "f3b216e7-8741-415e-fb57-2d141723bb4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… AWS Clients Initialized (S3, Transcribe, Rekognition, Bedrock)\n",
            "âœ… Firecrawl Client Initialized\n",
            "âœ… Groq Client Initialized\n",
            "\n",
            "Configuration loaded successfully.\n",
            "Input Source: s3://bytebytegonew/incoming/Screen_recording_50_min_DONE.mp4\n",
            "output.json initialized.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import boto3\n",
        "from firecrawl import FirecrawlApp\n",
        "from groq import Groq\n",
        "\n",
        "# 1. Load Credentials\n",
        "try:\n",
        "    with open(\"creds.json\", \"r\") as f:\n",
        "        creds = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: creds.json not found. Please run Step 5 and configure your keys.\")\n",
        "    creds = {}\n",
        "\n",
        "# 2. Extract Configuration Variables\n",
        "AWS_ACCESS_KEY = creds.get(\"AWS_ACCESS_KEY\")\n",
        "AWS_SECRET_KEY = creds.get(\"AWS_SECRET_KEY\")\n",
        "AWS_REGION = creds.get(\"AWS_REGION\", \"us-west-2\")\n",
        "BUCKET_NAME = creds.get(\"BUCKET_NAME\")\n",
        "VIDEO_S3_KEY = creds.get(\"VIDEO_S3_KEY\")\n",
        "BEDROCK_MODEL_ID = creds.get(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n",
        "\n",
        "CLOUDINARY_CLOUD = creds.get(\"CLOUDINARY_CLOUD\")\n",
        "CLOUDINARY_KEY = creds.get(\"CLOUDINARY_KEY\")\n",
        "CLOUDINARY_SECRET = creds.get(\"CLOUDINARY_SECRET\")\n",
        "FIRECRAWL_API_KEY = creds.get(\"FIRECRAWL_API_KEY\")\n",
        "GROQ_API_KEY = creds.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# 3. Initialize AWS Clients (using boto3)\n",
        "try:\n",
        "    boto3_session = boto3.Session(\n",
        "        aws_access_key_id=AWS_ACCESS_KEY,\n",
        "        aws_secret_access_key=AWS_SECRET_KEY,\n",
        "        region_name=AWS_REGION\n",
        "    )\n",
        "    s3_client = boto3_session.client('s3')\n",
        "    transcribe_client = boto3_session.client('transcribe')\n",
        "    rekog_client = boto3_session.client('rekognition')\n",
        "    bedrock_client = boto3_session.client('bedrock-runtime')\n",
        "    print(\"âœ… AWS Clients Initialized (S3, Transcribe, Rekognition, Bedrock)\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to initialize AWS clients: {e}\")\n",
        "\n",
        "# 4. Initialize External Service Clients\n",
        "try:\n",
        "    fc = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
        "    print(\"âœ… Firecrawl Client Initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to initialize Firecrawl: {e}\")\n",
        "\n",
        "try:\n",
        "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "    print(\"âœ… Groq Client Initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to initialize Groq: {e}\")\n",
        "\n",
        "# 5. Initialize Output State\n",
        "state = {\n",
        "    \"step_0_input\": {\n",
        "        \"source_s3_url\": f\"s3://{BUCKET_NAME}/{VIDEO_S3_KEY}\"\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"project\": \"Video-to-Knowledge Agent\",\n",
        "        \"status\": \"initialized\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)\n",
        "\n",
        "print(\"\\nConfiguration loaded successfully.\")\n",
        "print(f\"Input Source: {state['step_0_input']['source_s3_url']}\")\n",
        "print(\"output.json initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnn6c1RWtD5q",
        "outputId": "815f5b46-299e-485e-d055-367d4f0bd71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded. AWS and external service clients initialized.\n",
            "Initialized output.json with source video location.\n"
          ]
        }
      ],
      "source": [
        "# Step 0: Load configuration and initialize clients & state\n",
        "import json, os, boto3, requests\n",
        "from firecrawl import Firecrawl\n",
        "from groq import Groq\n",
        "\n",
        "# Load credentials from creds.json\n",
        "with open(\"creds.json\", \"r\") as f:\n",
        "    creds = json.load(f)\n",
        "\n",
        "# AWS configuration\n",
        "AWS_ACCESS_KEY = creds.get(\"AWS_ACCESS_KEY\")\n",
        "AWS_SECRET_KEY = creds.get(\"AWS_SECRET_KEY\")\n",
        "AWS_REGION = creds.get(\"AWS_REGION\", \"us-west-2\")\n",
        "BUCKET_NAME = creds.get(\"BUCKET_NAME\")\n",
        "VIDEO_S3_KEY = creds.get(\"VIDEO_S3_KEY\")\n",
        "BEDROCK_MODEL_ID = creds.get(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n",
        "\n",
        "# External service credentials\n",
        "CLOUDINARY_CLOUD = creds.get(\"CLOUDINARY_CLOUD\")\n",
        "CLOUDINARY_KEY = creds.get(\"CLOUDINARY_KEY\")\n",
        "CLOUDINARY_SECRET = creds.get(\"CLOUDINARY_SECRET\")\n",
        "FIRECRAWL_API_KEY = creds.get(\"FIRECRAWL_API_KEY\")\n",
        "GROQ_API_KEY = creds.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Configure boto3 to use provided AWS credentials\n",
        "boto3_session = boto3.Session(\n",
        "    aws_access_key_id=AWS_ACCESS_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "# Initialize AWS service clients\n",
        "s3_client = boto3_session.client('s3')\n",
        "transcribe_client = boto3_session.client('transcribe')\n",
        "rekog_client = boto3_session.client('rekognition')\n",
        "bedrock_client = boto3_session.client('bedrock-runtime')\n",
        "\n",
        "\n",
        "fc = Firecrawl(api_key=FIRECRAWL_API_KEY) # Firecrawl client for web scraping\n",
        "groq_client = Groq(api_key=GROQ_API_KEY) # Groq client for Whisper transcription\n",
        "\n",
        "# Prepare local directories (ensure they exist)\n",
        "os.makedirs(\"temp\", exist_ok=True)\n",
        "os.makedirs(\"frames/raw\", exist_ok=True)\n",
        "os.makedirs(\"frames/crops\", exist_ok=True)\n",
        "os.makedirs(\"external_data\", exist_ok=True)\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "# Initialize output state\n",
        "state = {\n",
        "    \"step_0_input\": {\n",
        "        \"source_s3_url\": f\"s3://{BUCKET_NAME}/{VIDEO_S3_KEY}\"\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"project\": \"Video-to-Knowledge Agent\",\n",
        "        \"status\": \"initialized\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)\n",
        "\n",
        "print(\"Configuration loaded. AWS and external service clients initialized.\")\n",
        "print(\"Initialized output.json with source video location.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33b69cf",
        "outputId": "10770c27-c9e0-44b3-e279-6c118fc5b147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading video from s3://bytebytegonew/incoming/Screen_recording_50_min_DONE.mp4 to temp/Screen_recording_50_min_DONE.mp4 ...\n",
            "Download complete. Video saved locally.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Ingestion (Download video from S3 to local)\n",
        "local_video_path = \"temp/video.mp4\"\n",
        "local_video_path = \"temp/Screen_recording_50_min_DONE.mp4\"\n",
        "src_bucket = BUCKET_NAME\n",
        "src_key = VIDEO_S3_KEY\n",
        "print(f\"Downloading video from s3://{src_bucket}/{src_key} to {local_video_path} ...\")\n",
        "s3_client.download_file(src_bucket, src_key, local_video_path)\n",
        "print(\"Download complete. Video saved locally.\")\n",
        "\n",
        "# Update state and output.json\n",
        "state[\"step_1_local\"] = {\"local_video_path\": local_video_path}\n",
        "state[\"metadata\"][\"status\"] = \"video_downloaded\"\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8618ea8f",
        "outputId": "d5484250-b41f-4d35-e106-040a24a46fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting audio from video...\n",
            "MoviePy - Writing audio in temp/extracted_audio.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Audio extracted to temp/extracted_audio.mp3\n",
            "Uploaded audio to s3://bytebytegonew/audio/extracted_audio.mp3\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Audio Extraction (Video -> MP3)\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "video_file = state[\"step_1_local\"][\"local_video_path\"]\n",
        "audio_output_path = \"temp/extracted_audio.mp3\"\n",
        "\n",
        "print(\"Extracting audio from video...\")\n",
        "clip = VideoFileClip(video_file)\n",
        "# Write audio to file in MP3 format\n",
        "clip.audio.write_audiofile(audio_output_path, codec=\"libmp3lame\")\n",
        "clip.close()\n",
        "print(f\"Audio extracted to {audio_output_path}\")\n",
        "\n",
        "# Upload the extracted audio to S3\n",
        "audio_s3_key = \"audio/extracted_audio.mp3\"\n",
        "s3_client.upload_file(audio_output_path, BUCKET_NAME, audio_s3_key)\n",
        "audio_s3_uri = f\"s3://{BUCKET_NAME}/{audio_s3_key}\"\n",
        "print(f\"Uploaded audio to {audio_s3_uri}\")\n",
        "\n",
        "# Update state and output.json\n",
        "state[\"step_2_audio\"] = {\n",
        "    \"audio_s3_uri\": audio_s3_uri,\n",
        "    \"local_audio_path\": audio_output_path\n",
        "}\n",
        "state[\"metadata\"][\"status\"] = \"audio_extracted\"\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c56f4b80",
        "outputId": "bbd1806e-42e8-406c-c6da-72cf91c909e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcribing with Groq Whisper API (Whisper large v3 model)...\n",
            "Groq transcription failed, falling back to AWS Transcribe: Error code: 413 - {'error': {'message': 'Request Entity Too Large', 'type': 'invalid_request_error', 'code': 'request_too_large'}}\n",
            "Transcribing with AWS Transcribe...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "Waiting for AWS Transcribe job to complete...\n",
            "AWS Transcribe job completed. Downloaded result JSON.\n",
            "Uploaded cleaned transcript to s3://bytebytegonew/transcripts/clean_transcript.txt\n",
            "Transcription step completed.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import uuid # Import uuid for generating unique job names\n",
        "\n",
        "audio_s3_uri = state[\"step_2_audio\"][\"audio_s3_uri\"]\n",
        "transcript_text = None\n",
        "USE_GROQ = True  # Attempt Groq Whisper API first for transcription\n",
        "\n",
        "if USE_GROQ:\n",
        "    print(\"Transcribing with Groq Whisper API (Whisper large v3 model)...\")\n",
        "    try:\n",
        "        # Groq transcription\n",
        "        with open(state[\"step_2_audio\"][\"local_audio_path\"], \"rb\") as f:\n",
        "            audio_data = f.read()\n",
        "        response = groq_client.audio.transcriptions.create(\n",
        "            file=(\"audio.mp3\", audio_data),\n",
        "            model=\"whisper-large-v3-turbo\",\n",
        "            response_format=\"text\"  # plain text output\n",
        "        )\n",
        "        transcript_text = response  # The groq client returns the text directly in this mode\n",
        "        print(\"Groq transcription complete.\")\n",
        "    except Exception as e:\n",
        "        print(\"Groq transcription failed, falling back to AWS Transcribe:\", e)\n",
        "        USE_GROQ = False\n",
        "\n",
        "if not USE_GROQ:\n",
        "    print(\"Transcribing with AWS Transcribe...\")\n",
        "    # Generate a unique job name using a timestamp or UUID\n",
        "    job_name = f\"Video2KnowledgeTranscriptionJob-{uuid.uuid4()}\"\n",
        "    media_uri = audio_s3_uri  # S3 URI for Transcribe to fetch\n",
        "    transcribe_client.start_transcription_job(\n",
        "        TranscriptionJobName=job_name,\n",
        "        Media={\"MediaFileUri\": media_uri},\n",
        "        MediaFormat=\"mp3\",\n",
        "        LanguageCode=\"en-US\",\n",
        "        OutputBucketName=BUCKET_NAME,  # Transcribe will put the result here\n",
        "        OutputKey=f\"transcripts/{job_name}.json\" # Use unique key for output as well\n",
        "    )\n",
        "\n",
        "    # Poll for completion\n",
        "    while True:\n",
        "        status = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
        "        status_val = status[\"TranscriptionJob\"][\"TranscriptionJobStatus\"]\n",
        "        if status_val in [\"COMPLETED\", \"FAILED\"]:\n",
        "            break\n",
        "        print(\"Waiting for AWS Transcribe job to complete...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    if status_val == \"COMPLETED\":\n",
        "        # Download the transcription result JSON from S3\n",
        "        result_key = f\"transcripts/{job_name}.json\"\n",
        "        result_local = \"temp/raw_transcript.json\" # This can remain static if overwritten\n",
        "        s3_client.download_file(BUCKET_NAME, result_key, result_local)\n",
        "        print(\"AWS Transcribe job completed. Downloaded result JSON.\")\n",
        "\n",
        "        # Parse out the transcript text\n",
        "        with open(result_local, \"r\") as f:\n",
        "            result_json = json.load(f)\n",
        "\n",
        "        try:\n",
        "            # AWS Transcribe JSON structure extraction\n",
        "            transcript_text = result_json[\"results\"][\"transcripts\"][0][\"transcript\"]\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing Transcribe output:\", e)\n",
        "            transcript_text = \"\"\n",
        "    else:\n",
        "        print(\"AWS Transcribe failed with status:\", status_val)\n",
        "        transcript_text = \"\"\n",
        "\n",
        "# Save cleaned transcript text to file and S3\n",
        "os.makedirs(\"transcripts\", exist_ok=True)\n",
        "transcript_txt_path = \"temp/clean_transcript.txt\"\n",
        "with open(transcript_txt_path, \"w\") as f:\n",
        "    f.write(transcript_text)\n",
        "\n",
        "# Upload the cleaned transcript text to S3\n",
        "transcript_s3_key = \"transcripts/clean_transcript.txt\"\n",
        "s3_client.upload_file(transcript_txt_path, BUCKET_NAME, transcript_s3_key)\n",
        "transcript_s3_uri = f\"s3://{BUCKET_NAME}/{transcript_s3_key}\"\n",
        "print(f\"Uploaded cleaned transcript to {transcript_s3_uri}\")\n",
        "\n",
        "# Update state and output.json\n",
        "state[\"step_3_transcription\"] = {\n",
        "    \"transcript_text_s3\": transcript_s3_uri,\n",
        "    \"transcript_text\": transcript_text[:1000]  # store a snippet for quick reference\n",
        "}\n",
        "state[\"metadata\"][\"status\"] = \"transcribed\"\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)\n",
        "\n",
        "print(\"Transcription step completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20b73703"
      },
      "source": [
        "### ðŸ’¡ Explanation: Transcription Complete\n",
        "\n",
        "We attempted transcription with Groq Whisper API first. The Groq client uses the Whisper large model to transcribe the audio; we requested plain text output for simplicity. If successful, this yields the transcript quickly without using any AWS resources (Groqâ€™s service is free for our use-case). If it fails (e.g., API issues or audio too large >25MB), we catch the exception and switch to AWS Transcribe.\n",
        "\n",
        "For AWS Transcribe, we used the asynchronous job API. The job is started with the audioâ€™s S3 URI and we poll until completion. Once done, we download the resulting JSON from S3. We then parse the JSON to extract the transcript text. This approach ensures we donâ€™t overload the notebook with large responses, and Transcribe writes the output to S3 for us. We then clean and save the text.\n",
        "\n",
        "The transcript text (cleaned) is uploaded to `transcripts/clean_transcript.txt` in S3 and also saved locally. The state is updated with the S3 path and a snippet of the text. Storing a snippet (e.g., first 1000 chars) in the state helps with quick verification without bloating the JSON. The status is updated to `\"transcribed\"`.\n",
        "\n",
        "Using Groq by default helps conserve AWS free minutes. Amazon Transcribeâ€™s free tier (if still applicable) covers the first hour per month, so either way this step should not incur cost for short videos. We now have the full video transcript ready for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT9Z5e5pLRQI"
      },
      "source": [
        "# Frame Extraction From Video\n",
        "\n",
        "\n",
        "Frames extracted from local video at 1 frame/5 second using OpenCV.\n",
        "Saved locally + uploaded to frames/raw/ in S3 for persistence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Txf073Jj45Qo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "\n",
        "def load_frames_every_n_seconds(video_path, step_seconds=1.0):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    print(f\"FPS: {fps}\")\n",
        "    step_frames = math.floor(fps * step_seconds)\n",
        "    print(f\"Step frames: {step_frames}\")\n",
        "\n",
        "    frames = []\n",
        "    frame_index = 0\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      if frame_index % step_frames == 0:\n",
        "        frames.append(frame.copy())\n",
        "      frame_index += 1\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHQrR0O26GBJ",
        "outputId": "243fa2fb-b8dc-495d-9693-71ed2090dd3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS: 57.26741837024141\n",
            "Step frames: 57\n",
            "Loaded 3005 sampled frames\n"
          ]
        }
      ],
      "source": [
        "video_path = state[\"step_1_local\"][\"local_video_path\"]\n",
        "original_frames = load_frames_every_n_seconds(video_path, step_seconds=1.0)\n",
        "print(f\"Loaded {len(original_frames)} sampled frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OTiccGKq-I59"
      },
      "outputs": [],
      "source": [
        "deduplicated_frame_paths = save_frames_to_folder(original_frames, \"frames_raw\")\n",
        "bucket_name = BUCKET_NAME\n",
        "s3_prefix = \"frames/raw/\" # Changed prefix to 'frames/dedup/' as per problem description for deduplicated frames\n",
        "s3_image_objects = upload_images_to_s3(deduplicated_frame_paths, bucket_name, s3_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5gcRKzdLqQy"
      },
      "source": [
        "# Keyframe Deduplication\n",
        "\n",
        "*Unique keyframes identified via SSIM comparison (threshold ~0.95).*\n",
        "\n",
        "After splitting the frames per second of the lecture recording. The reduction of the duplicates of the frames to save the space and the cost of the knowledge base and to reduce the noise of the dataset.\n",
        "\n",
        "There are some python libraries that apply the Convolutional Neural Network to create embedding vectors of each frames and find duplicates and can removed afterwards.\n",
        "\n",
        "For instance, the desplices and the imagededup are good examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtsqwZDa9GFC",
        "outputId": "c49900bf-c4a6-4f1f-e0bc-97df959f30fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 140MB/s]\n",
            "2025-12-31 13:01:30,883: INFO Device set to cuda ..\n",
            "INFO:imagededup.methods.cnn:Device set to cuda ..\n",
            "2025-12-31 13:01:31,059: INFO Initialized: mobilenet_v3_small for feature extraction ..\n",
            "INFO:imagededup.methods.cnn:Initialized: mobilenet_v3_small for feature extraction ..\n",
            "2025-12-31 13:01:31,060: INFO Start: Image encoding generation\n",
            "INFO:imagededup.methods.cnn:Start: Image encoding generation\n",
            "2025-12-31 13:01:45,835: INFO End: Image encoding generation\n",
            "INFO:imagededup.methods.cnn:End: Image encoding generation\n"
          ]
        }
      ],
      "source": [
        "from imagededup.methods import CNN\n",
        "import numpy as np\n",
        "encoder = CNN()  # uses mobilenet_v3_small by default\n",
        "image_dir_for_encoding = \"/content/frames_raw\"\n",
        "\n",
        "encodings = encoder.encode_images(image_dir=image_dir_for_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU8onMh8FzyF",
        "outputId": "742bae81-4bfa-4853-b4ae-7a80421f2b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified 28 unique keyframes (from 3005 raw frames).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def consine_similarity(a,b):\n",
        "  # Cosine distance: 1 - cosine_similarity. If embeddings are normalized, dot product is cosine similarity.\n",
        "  return 1.0 - np.dot(a, b)\n",
        "\n",
        "def l2_norm(a):\n",
        "  # Normalize vector to unit length\n",
        "  return a / (np.linalg.norm(a) + 1e-8) # Add small epsilon to prevent division by zero\n",
        "\n",
        "norm_encoding = {filename: l2_norm(v) for filename, v in encodings.items()}\n",
        "\n",
        "indexed_filenames = sorted(\n",
        "    [(int(filename.split('_')[1].split('.')[0]), filename) for filename in norm_encoding.keys()],\n",
        "    key=lambda x: x[0]\n",
        ")\n",
        "\n",
        "deduplicated_frames_actual = [] # This will store the actual NumPy array frames\n",
        "deduplicated_frame_info = [] # This will store info about the deduplicated frames for state update later\n",
        "\n",
        "last_emb = None\n",
        "last_frame_index_added = -1 # Keep track of the index of the last added frame\n",
        "\n",
        "DISTANCE_THRESHOLD = 0.1\n",
        "\n",
        "for frame_index, filename in indexed_filenames:\n",
        "  current_emb = norm_encoding[filename]\n",
        "  if last_emb is None:\n",
        "    # Always add the very first frame\n",
        "    deduplicated_frames_actual.append(original_frames[frame_index])\n",
        "    deduplicated_frame_info.append({\n",
        "        \"original_filename\": filename,\n",
        "        \"original_index\": frame_index,\n",
        "        \"timestamp_sec\": frame_index # Assuming 1 frame per second initially sampled\n",
        "    })\n",
        "    last_emb = current_emb\n",
        "    last_frame_index_added = frame_index\n",
        "  else:\n",
        "    dist = consine_similarity(last_emb, current_emb)\n",
        "    if dist > DISTANCE_THRESHOLD: # If the current frame is significantly different from the last added unique frame\n",
        "      deduplicated_frames_actual.append(original_frames[frame_index])\n",
        "      deduplicated_frame_info.append({\n",
        "          \"original_filename\": filename,\n",
        "          \"original_index\": frame_index,\n",
        "          \"timestamp_sec\": frame_index # Assuming 1 frame per second initially sampled\n",
        "      })\n",
        "      last_emb = current_emb\n",
        "      last_frame_index_added = frame_index\n",
        "\n",
        "print(f\"Identified {len(deduplicated_frames_actual)} unique keyframes (from {len(original_frames)} raw frames).\")\n",
        "\n",
        "\n",
        "deduplicated_frames_to_save = deduplicated_frames_actual # Renaming for clarity and consistency with next step\n",
        "state[\"step_5_deduplicated_frames_info\"] = deduplicated_frame_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HT4kPSFQZP3",
        "outputId": "9aa8cafc-a470-4263-8ac0-0e974f5eec85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 28 deduplicated frames locally to 'frames_deduplicated/' and uploaded to S3.\n",
            "{'bucket': 'bytebytegonew', 'key': 'frames/dedup/frame_000000.jpg', 's3_uri': 's3://bytebytegonew/frames/dedup/frame_000000.jpg'}\n",
            "{'bucket': 'bytebytegonew', 'key': 'frames/dedup/frame_000001.jpg', 's3_uri': 's3://bytebytegonew/frames/dedup/frame_000001.jpg'}\n",
            "{'bucket': 'bytebytegonew', 'key': 'frames/dedup/frame_000002.jpg', 's3_uri': 's3://bytebytegonew/frames/dedup/frame_000002.jpg'}\n",
            "{'bucket': 'bytebytegonew', 'key': 'frames/dedup/frame_000003.jpg', 's3_uri': 's3://bytebytegonew/frames/dedup/frame_000003.jpg'}\n",
            "{'bucket': 'bytebytegonew', 'key': 'frames/dedup/frame_000004.jpg', 's3_uri': 's3://bytebytegonew/frames/dedup/frame_000004.jpg'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "deduplicated_frame_paths = save_frames_to_folder(deduplicated_frames_to_save, \"frames_deduplicated\")\n",
        "bucket_name = BUCKET_NAME\n",
        "s3_prefix = \"frames/dedup/\" # This prefix is correct as per problem description for deduplicated frames\n",
        "s3_image_objects = upload_images_to_s3(deduplicated_frame_paths, bucket_name, s3_prefix)\n",
        "\n",
        "print(f\"Saved {len(deduplicated_frame_paths)} deduplicated frames locally to 'frames_deduplicated/' and uploaded to S3.\")\n",
        "for obj in s3_image_objects[:5]:\n",
        "    print(obj)\n",
        "\n",
        "state[\"step_5_deduplicated_frames_s3_objects\"] = s3_image_objects\n",
        "state[\"metadata\"][\"status\"] = \"frames_deduplicated\"\n",
        "with open(\"output.json\", \"w\") as f:\n",
        "    json.dump(state, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LugTuzukSA"
      },
      "source": [
        "# External Data Scraping**  \n",
        "\n",
        "Direct HTTP requests â†’ **Firecrawl fallback** for dynamic content.  \n",
        "Scraped Markdown/text saved locally + uploaded to S3.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULpeQnmuunuk"
      },
      "outputs": [],
      "source": [
        "from firecrawl import Firecrawl as fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RPzIB_TSGC-"
      },
      "outputs": [],
      "source": [
        "results = fc.search(\n",
        "    query=\"What is the population of Berlin, Germany?\",\n",
        "    limit=2,\n",
        "    scrape_options={\"formats\": [\"markdown\", \"links\"]},\n",
        ")\n",
        "\n",
        "import pprint\n",
        "pprint.pp(results)\n",
        "print(type(results))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtXtgcmYXK0y"
      },
      "outputs": [],
      "source": [
        "from firecrawl import Firecrawl\n",
        "from langchain_core.tools import StructuredTool\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "class FirecrawlSearchInput(BaseModel):\n",
        "    query: str = Field(description=\"User's information need as a web search query.\")\n",
        "    limit: int = Field(description=\"Max number of pages to retrieve.\", default=3)\n",
        "\n",
        "@tool(\"web_search\")\n",
        "def web_search(query: str, limit: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Use Firecrawl's search endpoint to get markdown content for the query.\n",
        "    Returns a single big markdown string as context.\n",
        "    \"\"\"\n",
        "    results = firecrawl.search(\n",
        "        query=query,\n",
        "        limit=limit,\n",
        "        scrape_options={\"formats\": [\"markdown\", \"links\"]},\n",
        "    )\n",
        "    print(results)\n",
        "\n",
        "    contexts = []\n",
        "    contexts.append(results)\n",
        "    return contexts\n",
        "\n",
        "firecrawl_tool = StructuredTool.from_function(\n",
        "    name=\"web_search\",\n",
        "    description=\"Search the web and return summarized markdown content.\",\n",
        "    func=web_search,\n",
        "    args_schema=FirecrawlSearchInput,\n",
        ")\n",
        "print(web_search(\"What is the weather today in Berlin,Germany?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9zo8n6OTs11"
      },
      "source": [
        "### Knowledge Synthesis \n",
        "Combines transcript + external texts + diagrams into comprehensive prompt.\n",
        "ChatGPT model generates structured Markdown \"Study Guide\" â†’ S3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWeRJvaPwbl1"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "def read_text_from_s3(bucket: str, key: str) -> str:\n",
        "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    return obj[\"Body\"].read().decode(\"utf-8\")\n",
        "\n",
        "def read_bytes_from_s3(bucket: str, key: str) -> bytes:\n",
        "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    return obj[\"Body\"].read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNGEAXkYwsw8"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple\n",
        "\n",
        "def load_lecture_assets_from_s3(bucket: str, prefix: str) -> Tuple[str, str, List[Dict]]:\n",
        "\n",
        "    transcript = \"\"\n",
        "    notes = \"\"\n",
        "    images = []\n",
        "\n",
        "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            key = obj[\"Key\"]\n",
        "\n",
        "            if key.endswith(\"/\"):\n",
        "                continue\n",
        "\n",
        "            lower = key.lower()\n",
        "\n",
        "            # Transcript\n",
        "            if \"transcript\" in lower and lower.endswith(\".txt\"):\n",
        "                transcript = read_text_from_s3(bucket, key)\n",
        "\n",
        "            # Notes / explanations\n",
        "            elif (\"notes\" in lower or \"explanations\" in lower) and (\n",
        "                lower.endswith(\".txt\") or lower.endswith(\".md\")\n",
        "            ):\n",
        "                notes = read_text_from_s3(bucket, key)\n",
        "\n",
        "            # Images (screenshots / diagrams)\n",
        "            elif lower.endswith(\".png\") or lower.endswith(\".jpg\") or lower.endswith(\".jpeg\"):\n",
        "                img_bytes = read_bytes_from_s3(bucket, key)\n",
        "                fmt = \"png\" if lower.endswith(\".png\") else \"jpeg\"\n",
        "                images.append({\"key\": key, \"bytes\": img_bytes, \"format\": fmt})\n",
        "\n",
        "    return transcript, notes, images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UDIt7dAUkT2G"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from typing import List, Dict, Tuple\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def build_multimodal_lecture_message(\n",
        "    transcript: str,\n",
        "    notes: str,\n",
        "    images: List[Dict],\n",
        "    max_images: int = 4,\n",
        ") -> HumanMessage:\n",
        "    \"\"\"\n",
        "    Turn transcript + notes + raw image bytes into a single multimodal HumanMessage\n",
        "    that GPT-4o (via LangChain) can understand.\n",
        "    \"\"\"\n",
        "    # 1. Build the text context (you can customise this)\n",
        "    lecture_text = \"\"\n",
        "    if transcript:\n",
        "        lecture_text += \"TRANSCRIPT:\\n\" + transcript + \"\\n\\n\"\n",
        "    if notes:\n",
        "        lecture_text += \"ADDITIONAL NOTES / EXPLANATIONS:\\n\" + notes + \"\\n\\n\"\n",
        "\n",
        "    if not lecture_text:\n",
        "        lecture_text = \"No transcript or notes were provided.\\n\\n\"\n",
        "\n",
        "    content = [\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": (\n",
        "                \"You will receive a lecture transcript, optional notes, and one or more slide images.\\n\"\n",
        "                \"Use all of this context together to produce high-quality notes.\\n\\n\"\n",
        "                f\"{lecture_text}\"\n",
        "            ),\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 2. Attach images as base64 inline data URIs\n",
        "    for img in images[:max_images]:\n",
        "        fmt = img[\"format\"]  # \"png\" or \"jpeg\"\n",
        "        b64 = base64.b64encode(img[\"bytes\"]).decode(\"utf-8\")\n",
        "        data_url = f\"data:image/{fmt};base64,{b64}\"\n",
        "        content.append(\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": data_url},\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return HumanMessage(content=content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TXT8Cba8kbcp"
      },
      "outputs": [],
      "source": [
        "transcript, notes, images = load_lecture_assets_from_s3(\"bytebytegonew\", \"list_objects_v2\")\n",
        "user_msg = build_multimodal_lecture_message(transcript, notes, images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQA1AAV9yfJ1"
      },
      "outputs": [],
      "source": [
        "api_key = os.environ[\"OPEN_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYVwgCWVlFM7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",  \n",
        "    temperature=0.3,\n",
        "    api_key=api_key,\n",
        ")\n",
        "\n",
        "system_instruction = \"\"\"You are an expert university-level teaching assistant and note-taker. Your task is to transform a lecture recording into clear, in-depth, well-structured notes that a student can use to revise and understand the material later. You will be given:\n",
        "- A transcript of the lecture (possibly noisy)\n",
        "- Optionally, extracted slide text or key screenshots (as text)\n",
        "\n",
        "GENERAL RULES\n",
        "-------------\n",
        "- Write as if for a motivated MSc-level student who wants to deeply understand and revisit the material.\n",
        "- Be accurate and grounded.\n",
        "- If something is unclear or missing in the transcript, do NOT hallucinate. Mark it clearly as â€œ[unclear in audio]â€ or â€œ[not covered in lecture]â€.\n",
        "- Preserve important technical details (definitions, formulas, symbols, code snippets), but rewrite them in a clear and concise way.\n",
        "- Use consistent terminology throughout.\n",
        "- Assume the user will read this as standalone notes without the transcript.\n",
        "\n",
        "OUTPUT FORMAT\n",
        "-------------\n",
        "Return your answer in Markdown with the following structure:\n",
        "\n",
        "1. **Title & Metadata**\n",
        "   - Course\n",
        "   - Lecture\n",
        "   - Date (if given)\n",
        "   - Lecturer (if given)\n",
        "\n",
        "2. **High-Level Summary (10â€“20 bullet points)**\n",
        "   ...\n",
        "\n",
        "7. **Summary & Key Takeaways**\n",
        "8. **Self-Check Questions**\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_instruction),\n",
        "        (\"human\",\n",
        "         \"Here is the lecture content (transcript + slides etc.):\\n\"\n",
        "         \"{lecture_context}\\n\\n\"\n",
        "         \"Task: {task}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "system_msg = SystemMessage(content=system_instruction)\n",
        "\n",
        "response = llm.invoke([system_msg, user_msg])  # user_msg = HumanMessage built above\n",
        "print(response.content)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
